{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H96kPkzqf0B"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VbB9_hdqhkw"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import ale_py       \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2YW1svvqjvG"
      },
      "source": [
        "# Partie 1: Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQWOl2XweTt"
      },
      "source": [
        "Nous allons commencer notre TP avec un environnement type grille: le [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake). Cela vous aidera également à vous familiariser avec l'API Gym, largement utilisée de le monde du RL.\n",
        "\n",
        "Commencez par lire la [documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake) de l'environnement Frozen Lake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IgXHaBg4p-N"
      },
      "source": [
        "## Print utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRonTcLQ4scp"
      },
      "outputs": [],
      "source": [
        "def int_action_to_str(int_action):\n",
        "    if int_action == 0:\n",
        "        return \"left\"\n",
        "    if int_action == 1:\n",
        "        return \"down\"\n",
        "    if int_action == 2:\n",
        "        return \"right\"\n",
        "    else:\n",
        "      return \"up\"\n",
        "\n",
        "for i in range(4):\n",
        "    print(f\"Corresponding action to {i} : {int_action_to_str(i)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx6ZY5XS3bpU"
      },
      "outputs": [],
      "source": [
        "def print_values(values, grid_width):\n",
        "    for i in range(int(len(values)/grid_width)):\n",
        "        print(values[i*grid_width:(i+1)*grid_width])\n",
        "\n",
        "print(f\"Example of printing values in a grid of width 4:\")\n",
        "print_values(np.arange(16), 4)\n",
        "\n",
        "def print_policy(policy, grid_width):\n",
        "    for i in range(int(len(policy)/grid_width)):\n",
        "        print([int_action_to_str(_a) for _a in policy[i*grid_width:(i+1)*grid_width]])\n",
        "\n",
        "print()\n",
        "print(f\"Example of printing policy in a grid of width 4:\")\n",
        "print_policy(np.arange(16)%4, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L46vWeKZ5j0K"
      },
      "outputs": [],
      "source": [
        "def print_q_values(q_values, grid_width, precision : int = 2):\n",
        "    for i in range(int(len(q_values)/grid_width)):\n",
        "        _q_values_to_print = []\n",
        "        for _q_values in q_values[i*grid_width:(i+1)*grid_width]:\n",
        "            _q_values_to_print.append({int_action_to_str(_k): round(_v, precision).item() for _k, _v in enumerate(_q_values)})\n",
        "        print(_q_values_to_print)\n",
        "        \n",
        "print(f\"Example of printing q_values in a grid of width 4:\")\n",
        "print_q_values(np.random.rand(16, 4), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_jZtNFw5FS"
      },
      "source": [
        "## A) Version déterministe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZV144o4ynmt"
      },
      "source": [
        "Créons une première instance de l'environnement avec une carte spécifique:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTqEgIM_vPn1"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\n",
        "    'FrozenLake-v1', \n",
        "    desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"], \n",
        "    is_slippery=False, \n",
        "    render_mode=\"rgb_array\",   # \"rgb_array\" produces RGB frames to plot, \"human\" opens a window (doesn't work on notebooks)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zpIjUdRyt1F"
      },
      "source": [
        "Il est nécessaire de reset l'environnement pour lancer un épisode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FffA5N7WvNaU"
      },
      "outputs": [],
      "source": [
        "obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "BhUo2TnTvaVV",
        "outputId": "3ae51d03-79fa-458a-8594-e4f46933b925"
      },
      "outputs": [],
      "source": [
        "img = env.render()\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cYGke80yzjr"
      },
      "source": [
        "Regardez la fonction de transition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDjtbUgAw_ns"
      },
      "outputs": [],
      "source": [
        "def print_transition_function(env, state, action, ):\n",
        "    print(f\"From state {state} when playing action {action}:\")\n",
        "    for next_state_transition in env.unwrapped.P[state][action]:\n",
        "        print(f\"- Reaching state {next_state_transition[1]} along with reward {next_state_transition[2]} with probability {next_state_transition[0]}\")\n",
        "        \n",
        "# State: initial\n",
        "# Action: right\n",
        "print_transition_function(env, state=0, action=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUz6TL6bzBmy",
        "outputId": "86a7b9df-2e4b-4dce-b649-3c351a845fba"
      },
      "outputs": [],
      "source": [
        "# State: cell 4 (next ot a hole)\n",
        "# Action: right\n",
        "print_transition_function(env, state=4, action=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01NvxPuNznHM",
        "outputId": "06efe21f-f385-4745-8727-1669220ad453"
      },
      "outputs": [],
      "source": [
        "# State: cell 14 (next to the goal)\n",
        "# Action: right\n",
        "print_transition_function(env, state=14, action=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cette classe permet d'aggréger les frames des env.render() puis d'afficher l'épisode à la fin. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "class EpisodeAnimator:\n",
        "    def __init__(self, interval: int = 200):\n",
        "        \"\"\"\n",
        "        interval: delay between frames in milliseconds\n",
        "        \"\"\"\n",
        "        self.frames = []\n",
        "        self.interval = interval\n",
        "    \n",
        "    def add_frame(self, frame):\n",
        "        \"\"\"Add a single frame (numpy RGB array) to the animation.\"\"\"\n",
        "        self.frames.append(frame)\n",
        "    \n",
        "    def display_animation(self):\n",
        "        \"\"\"Render and display the animation inline in a Jupyter notebook.\"\"\"\n",
        "        if not self.frames:\n",
        "            raise ValueError(\"No frames to animate. Use add_frame() first.\")\n",
        "        \n",
        "        fig = plt.figure()\n",
        "        img = plt.imshow(self.frames[0])\n",
        "        plt.axis('off')\n",
        "\n",
        "        def animate(i):\n",
        "            img.set_data(self.frames[i])\n",
        "            return [img]\n",
        "        \n",
        "        ani = animation.FuncAnimation(\n",
        "            fig, animate, frames=len(self.frames),\n",
        "            interval=self.interval, blit=True\n",
        "        )\n",
        "        plt.close(fig)  # Prevent static last frame from showing\n",
        "        return HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cette fonction joue (évalue) une certaine politique dans un certain environnement précisés pendant 1 épisode, et retourne les récompenses cumulées ainsi que les frames de l'épisode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "\n",
        "def eval_policy_1_episode(env : gym.Env, policy : Callable, do_animation : bool = True):\n",
        "    observation, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step = 0\n",
        "    animator = EpisodeAnimator(interval=300) if do_animation else None\n",
        "    \n",
        "    while not done:\n",
        "        \n",
        "        if do_animation:\n",
        "            animator.add_frame(env.render())\n",
        "\n",
        "        action = policy(observation)\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "    \n",
        "    if do_animation:\n",
        "        animator.add_frame(env.render()) # add last frame\n",
        "        animation = animator.display_animation()\n",
        "    else:\n",
        "        animation = None\n",
        "        \n",
        "    return {\"total_reward\": total_reward, \"steps\": step, \"animation\" : animation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPAwykBF0PSJ"
      },
      "source": [
        "Codez désormais une stratégie permettant de résoudre le maze:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_hardcoded(observation):\n",
        "    # Implement a hardcoded policy\n",
        "    if observation in [0, 4, 10]:\n",
        "        action = 1\n",
        "    else:\n",
        "        action = 2\n",
        "    return action\n",
        "\n",
        "result = eval_policy_1_episode(env, policy_hardcoded, do_animation=True)\n",
        "print(f\"Total reward: {result['total_reward']}\")\n",
        "print(f\"Total steps: {result['steps']}\")\n",
        "print(f\"Animation:\")\n",
        "result['animation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmO-4tyZ11aU"
      },
      "source": [
        "Testez à nouveau votre stratégie avec cette nouvelle carte. Que fait-elle ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Kk0iWZsb1fku",
        "outputId": "08d9229d-8bd8-45d0-827e-3c6d9c1b2f99"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SHFF\", \"FHFH\", \"FFHF\", \"FFFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "\n",
        "result = eval_policy_1_episode(env, policy_hardcoded, do_animation=True)\n",
        "\n",
        "print(f\"Total reward: {result['total_reward']}\")\n",
        "print(f\"Total steps: {result['steps']}\")\n",
        "print(f\"Animation:\")\n",
        "result['animation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaxHKJz23WiN"
      },
      "source": [
        "## B) Version stochastique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3wORcs-3ohn"
      },
      "source": [
        "Passons maintenant à une verison stochastique où notre agent peut \"glisser\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1jI-f_Q2Srq"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"], is_slippery=True, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "5xyL4WDm3tyS",
        "outputId": "fcfe84e2-3f66-49b3-b453-06b59a1221ec"
      },
      "outputs": [],
      "source": [
        "result = eval_policy_1_episode(env, policy_hardcoded, do_animation=True)\n",
        "\n",
        "print(f\"Total reward: {result['total_reward']}\")\n",
        "print(f\"Total steps: {result['steps']}\")\n",
        "print(f\"Animation:\")\n",
        "result['animation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKhcRQID305z"
      },
      "source": [
        "Regardons ce que cela change sur la fonction de transition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI32zkVy36eD",
        "outputId": "6ddefb47-8e13-4006-fa43-458d4f8ae5df"
      },
      "outputs": [],
      "source": [
        "# State: initial\n",
        "# Action: right\n",
        "print_transition_function(env, state=0, action=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xogdKNsG36eE",
        "outputId": "f098533a-b848-4b74-a711-255adf289255"
      },
      "outputs": [],
      "source": [
        "# State: cell 4 (next ot a hole)\n",
        "# Action: right\n",
        "print_transition_function(env, state=4, action=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQwMvfTj36eF",
        "outputId": "0a826c41-9e11-41b0-dab6-8bc2a4c2e03d"
      },
      "outputs": [],
      "source": [
        "# State: cell 14 (next to the goal)\n",
        "# Action: right\n",
        "print_transition_function(env, state=14, action=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ConoxBZ4HMw"
      },
      "source": [
        "Essayez maintenant de trouver une stratégie sûre pour la carte suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "AC8vN2VL4br_",
        "outputId": "9da3d786-3afb-4bec-a330-82b396c0f05b"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FFFF\", \"HHFF\", \"HHFG\"], is_slippery=True, render_mode=\"rgb_array\")\n",
        "obs, info = env.reset()\n",
        "array = env.render()\n",
        "plt.imshow(array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SioJQuoT4v2j"
      },
      "source": [
        "Votre stratégie sera testée plusieurs fois pour s'assurer de sa fiabilité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_policy_n_episodes(env, policy, n=5, do_animation=True):\n",
        "    \"\"\"Evaluate a policy over multiple episodes.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment to evaluate the policy on.\n",
        "        policy (Callable): the policy to evaluate : policy(observation) -> action\n",
        "        n (int, optional): The number of episodes to evaluate the policy on. Defaults to 5.\n",
        "        do_animation (bool, optional): Whether to render the environment during evaluation. Only the last episode is rendered. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the evaluation results.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    for i in tqdm(range(n)):\n",
        "        result_episode = eval_policy_1_episode(env, policy, do_animation=do_animation and i == n-1)\n",
        "        for key, value in result_episode.items():\n",
        "            if key not in results:\n",
        "                results[key] = []\n",
        "            results[key].append(value)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y60aCxFw33km"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFFF\", \"FFFF\", \"HHFF\", \"HHFG\"], is_slippery=True, render_mode=\"rgb_array\")\n",
        "\n",
        "def policy_hardcoded_2(observation):\n",
        "    # ---- <your code here> ----\n",
        "    action = ...\n",
        "    # --------------------------\n",
        "    return action\n",
        "\n",
        "results = eval_policy_n_episodes(env, policy_hardcoded, n=10, do_animation=True)\n",
        "\n",
        "print(f\"Total reward: {results['total_reward']}\")\n",
        "print(f\"Total steps: {results['steps']}\")\n",
        "print(f\"Animation (last episode):\")\n",
        "results['animation'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUK2H8o_Rv5s"
      },
      "source": [
        "# Partie 3: Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Framework for Reinforcement Learning Agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pour faire les choses proprement nous allons définir une classe de base pour les agents de RL.\n",
        "Cette classe définit un interface python à respecter pour tout agent de RL. Les méthodes suivantes doivent être implémentées :\n",
        "\n",
        "- `__init__(self, env: gym.Env, **kwargs)`: Le constructeur prend un environnement OpenAI Gym comme entrée (et d'autre paramètres dépendant de l'agent si besoin)\n",
        "- `act(self, obs)`: Cette méthode doit retourner l'action à entreprendre en fonction de l'observation de l'état actuel.\n",
        "- `learn(self, obs, action, reward, next_obs, done)`: Cette méthode doit mettre à jour la politique de l'agent en fonction de l'expérience acquise sur la dernière transition. La variable `done` indique si l'épisode a été terminé lors de cette transition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentRL:\n",
        "    \n",
        "    def __init__(self, env : gym.Env):\n",
        "        self.env = env\n",
        "    \n",
        "    def act(self, obs):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un exemple d'agent (aléatoire) respectant ce framework :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomAgent(AgentRL):\n",
        "    \n",
        "    def act(self, obs):\n",
        "        return self.env.action_space.sample()\n",
        "\n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        pass  # do nothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un autre exemple d'agent basé sur une fonction politique spécifiée :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HardcodedAgent(AgentRL):\n",
        "    \n",
        "    def __init__(self, env : gym.Env, policy : Callable):\n",
        "        super().__init__(env)\n",
        "        self.policy = policy\n",
        "\n",
        "    def act(self, obs):\n",
        "        return self.policy(obs)\n",
        "\n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        pass  # do nothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Une fois votre agent implémenté, ces deux fonctions serviront respectivement à l'entraîner et à l'évaluer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agent_n_episodes(env : gym.Env, agent : AgentRL, n : int = 20, verbose_freq : int = None) -> dict:\n",
        "    \"\"\"Train the agent for a number of episodes.\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): The environment to train the agent in.\n",
        "        agent (AgentRL): The agent to train.\n",
        "        n (int, optional): The number of episodes to train the agent for. Defaults to 20.\n",
        "        verbose_freq (int, optional): Episode frequency to print progress. If None, no progress is printed. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the total rewards and steps taken in each episode.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        \"total_rewards\": [],\n",
        "        \"steps\": [],\n",
        "    }\n",
        "    \n",
        "    # Training loop\n",
        "    for episode in tqdm(range(n)):\n",
        "        observation, info = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(observation)\n",
        "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            total_reward += reward\n",
        "            agent.learn(observation, action, reward, next_observation, done)\n",
        "            step += 1\n",
        "            observation = next_observation\n",
        "        \n",
        "        # Log results\n",
        "        results[\"total_rewards\"].append(total_reward)\n",
        "        results[\"steps\"].append(step)\n",
        "        if verbose_freq is not None and (episode + 1) % verbose_freq == 0:\n",
        "            avg_reward = np.mean(results[\"total_rewards\"][-verbose_freq:])\n",
        "            avg_steps = np.mean(results[\"steps\"][-verbose_freq:])\n",
        "            print(f\"Episode {episode + 1}/{n} - Average Reward: {avg_reward:.2f}, Average Steps: {avg_steps:.2f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def eval_agent_n_episodes(env : gym.Env, agent : AgentRL, n : int = 5, do_animation : bool = True) -> dict:\n",
        "    return eval_policy_n_episodes(env, agent.act, n=n, do_animation=do_animation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHt-aUf0-Vjq"
      },
      "source": [
        "Pour commencer, nous utiliserons cet environnement très simple et sans trous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBprdZAxfD_Z"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFF\", \"FFF\", \"FFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "obs, info = env.reset()\n",
        "env.render()\n",
        "plt.imshow(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A) Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implémentez à présent votre premier algorithme : le Q-Learning.\n",
        "\n",
        "Rappel : l'algorithme de Q-Learning garde en mémoire une table de Q-values estimées $\\hat{Q}$ qui à chaque couple d'état action $(s,a)$ associe une valeur $\\hat{Q}(s,a)$ représentant une estimation de la valeur future de l'état $s$ après avoir effectué l'action $a$. Il met à jour cette table en utilisant la formule suivante :\n",
        "\n",
        "$$\\hat{Q}(s,a) \\leftarrow \\hat{Q}(s,a) + \\alpha \\left( r + \\gamma \\max_{a'} \\hat{Q}(s',a') - \\hat{Q}(s,a) \\right)$$\n",
        "\n",
        "où :\n",
        "- $\\alpha$ est le taux d'apprentissage,\n",
        "- $r$ est la récompense reçue après avoir effectué l'action $a$ dans l'état $s$,\n",
        "- $s'$ est l'état résultant de l'action $a$,\n",
        "- $\\gamma$ est le discount factor.\n",
        "\n",
        "Note : dans un état terminal (indiqué par la valeur de la variable `done`), la valeur de $\\hat{Q}(s',a')$ est nulle car il n'y a pas d'état suivant. Une formule plus correcte pour la mise à jour dans ce cas est :\n",
        "\n",
        "$$\\hat{Q}(s,a) \\leftarrow \\hat{Q}(s,a) + \\alpha \\left( r - \\hat{Q}(s,a) \\right)$$\n",
        "\n",
        "### Conseils :\n",
        "\n",
        "- Vous aurez besoin de maintenir une table de Q-values $\\hat{Q}$ pour chaque paire d'état-action $(s,a)$, initialisés arbitrairement.\n",
        "- N'oubliez pas la notion d'exploration : vous devez parfois choisir une action aléatoire plutôt que celle qui maximise la Q-value estimée, afin d'explorer de nouvelles régions de l'espace d'état/actions.\n",
        "- Le nombre d'actions et d'état existant est accessible par `env.action_space.n` et `env.observation_space.n`.\n",
        "- A vous de trouver de bonnes valeurs d'hyperparamètres (pour $\\alpha$, $\\gamma$, et $\\epsilon$).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QLearningAgent(AgentRL):\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        env : gym.Env, \n",
        "        alpha : float,\n",
        "        gamma : float,\n",
        "        epsilon : float,\n",
        "        ):\n",
        "        super().__init__(env)\n",
        "        # ---- <your code here> ----\n",
        "        \n",
        "        # --------------------------\n",
        "        \n",
        "    def act(self, obs):\n",
        "        # ---- <your code here> ----\n",
        "        \n",
        "        # --------------------------\n",
        "        pass\n",
        "    \n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        # ---- <your code here> ----\n",
        "        \n",
        "        # --------------------------\n",
        "        pass\n",
        "    \n",
        "agent = QLearningAgent(\n",
        "    env = env,\n",
        "    # ---- <your code here> ----\n",
        "    \n",
        "    # --------------------------\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Train for 2 episodes here to debug\n",
        "train_results = train_agent_n_episodes(\n",
        "    env = env,\n",
        "    agent = agent,\n",
        "    n = 2,\n",
        ")\n",
        "print(train_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrainez ensuite votre agent Q-Learning sur l'environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"SFF\", \"FFF\", \"FFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "agent = agent\n",
        "\n",
        "train_results = train_agent_n_episodes(\n",
        "    env = env,\n",
        "    agent = agent,\n",
        "    n = 200,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Après entrainement, vous pouvez afficher les courbes d'apprentissage depuis `train_results`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(results: dict):\n",
        "    \"\"\"\n",
        "    Plots each metric in results on a separate subplot.\n",
        "    Example results:\n",
        "        {\n",
        "            'total_rewards': [0.0, 1.0, 0.0],\n",
        "            'steps': [100, 95, 100]\n",
        "        }\n",
        "    \"\"\"\n",
        "    n_metrics = len(results)\n",
        "    fig, axes = plt.subplots(n_metrics, 1, figsize=(10, 4 * n_metrics), sharex=True)\n",
        "\n",
        "    if n_metrics == 1:\n",
        "        axes = [axes]  # Make iterable if only one metric\n",
        "\n",
        "    for ax, (key, values) in zip(axes, results.items()):\n",
        "        ax.plot(values, label=key)\n",
        "        ax.set_title(key.replace(\"_\", \" \").title())\n",
        "        ax.set_ylabel(\"Value\")\n",
        "        ax.grid(True)\n",
        "        ax.legend()\n",
        "\n",
        "    axes[-1].set_xlabel(\"Episode\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results(train_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluons à présent notre agent.\n",
        "\n",
        "Remarque (facultatif) : il pourrait être pertinent de passer l'agent en mode évaluation lorsqu'il n'est pas en cours d'entraînement. Ici cela correspondrait à désactiver l'exploration (c'est-à-dire, utiliser une politique déterministe)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_results = eval_agent_n_episodes(\n",
        "    env = env,\n",
        "    agent = agent,\n",
        "    n = 10,\n",
        "    )\n",
        "\n",
        "print(f\"Total reward: {eval_results['total_reward']}, mean = {np.mean(eval_results['total_reward'])}\")\n",
        "print(f\"Total steps: {eval_results['steps']}, mean = {np.mean(eval_results['steps'])}\")\n",
        "print(f\"Animation (last episode):\")\n",
        "eval_results['animation'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B) SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implémentons à présent SARSA, dont la formule d'apprentissage est : $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma Q(s', a') - Q(s, a) \\right)$\n",
        "\n",
        "Vous aurez donc à prendre $a' = \\pi(s')$ dans votre code, plutôt que de prendre le maximum de $Q(s', \\cdot)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define environment\n",
        "env = gym.make('FrozenLake-v1', desc=[\"SFF\", \"FFF\", \"FFG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "\n",
        "# Define agent\n",
        "class SARSA_Agent(AgentRL):\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        env : gym.Env, \n",
        "        alpha : float,\n",
        "        gamma : float,\n",
        "        epsilon : float,\n",
        "        ):\n",
        "        \n",
        "        super().__init__(env)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "        self.last_obs = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def act(self, obs):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = np.random.choice(self.env.action_space.n)\n",
        "        else:\n",
        "            action = np.argmax(self.q_table[obs])\n",
        "        return action\n",
        "\n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            next_action = self.act(next_obs)\n",
        "            target = reward + self.gamma * self.q_table[next_obs, next_action]\n",
        "        self.q_table[obs, action] += self.alpha * (target - self.q_table[obs, action])\n",
        "\n",
        "agent = SARSA_Agent(\n",
        "    env=env,\n",
        "    alpha=0.1,\n",
        "    gamma=0.9,\n",
        "    epsilon=0.1\n",
        ")\n",
        "\n",
        "# Train the agent\n",
        "training_results = train_agent_n_episodes(env, agent, n=200)\n",
        "\n",
        "# Plot training curves\n",
        "plot_results(training_results)\n",
        "\n",
        "# Evaluate the agent\n",
        "eval_results = eval_agent_n_episodes(env, agent, n=10)\n",
        "\n",
        "print(f\"Total reward: {eval_results['total_reward']}, mean = {np.mean(eval_results['total_reward'])}\")\n",
        "print(f\"Total steps: {eval_results['steps']}, mean = {np.mean(eval_results['steps'])}\")\n",
        "print(f\"Animation (last episode):\")\n",
        "eval_results['animation'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tLxoL5f0Xqz"
      },
      "source": [
        "## C) Environnement plus difficile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2gaAaYa93_2"
      },
      "source": [
        "Entrainons nos agents Q-Learning et SARSA dans l'environnement \"The Cliff\" du cours. \n",
        "\n",
        "Cet environnement est en l'état assez difficile parce que le signal de récompense est très sparse. L'agent ne devrait pas réussir normalement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSjwi_rT0e2-"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', desc=[\"FFFFF\", \"FFFFF\", \"SHHHG\"], is_slippery=False, render_mode=\"rgb_array\")\n",
        "obs, info = env.reset()\n",
        "img = env.render()\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = QLearningAgent(env, alpha=0.8, gamma=0.99, epsilon=0.33)\n",
        "\n",
        "# Train the agent\n",
        "training_results = train_agent_n_episodes(env, agent, n=1000)\n",
        "\n",
        "# Plot training curves\n",
        "plot_results(training_results)\n",
        "\n",
        "# Evaluate the agent\n",
        "eval_results = eval_agent_n_episodes(env, agent , n=10)\n",
        "print(f\"Total reward: {eval_results['total_reward']}, mean = {np.mean(eval_results['total_reward'])}\")\n",
        "print(f\"Total steps: {eval_results['steps']}, mean = {np.mean(eval_results['steps'])}\")\n",
        "print(f\"Animation (last episode):\")\n",
        "eval_results['animation'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comment rendre cet environnement plus facile à apprendre pour notre agent ? Le problème est que l'agent doit explorer beaucoup avant de tomber sur un signal de reward (lorsqu'il arrive au coffre). Toutes les autres trajectoires offrent une récompense totale nulle, et l'agent va en pratique très souvent tomber dans les trous (qui terminent l'épisode avec une reward nulle) et est donc bloqué.\n",
        "\n",
        "Les solutions proposés sont les suivantes :\n",
        "- 1) Rendre punitif les trous, pour cela on peut utiliser un \"wrapper\" d'environnement. Les wrapper sont des classes qui modifient le comportement d'un environnement Gym sans en changer le code source. Dans notre cas, on peut penser à donner une récompense négative lorsque l'épisode est terminé (`done=True`) mais qu'il n'a pas trouvé le coffre (`reward` < 1). Vous pouvez vous inspirer du code suivant (double click sur cette cellule pour copier le code) :\n",
        "\n",
        "```python\n",
        "class PunishTerminationWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Punishes any termination that doesn't give +1 reward with -1.\n",
        "    \"\"\"\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        \n",
        "        # ---- <your code here> ----\n",
        "        \n",
        "        # --------------------------\n",
        "        \n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "env = PunishTerminationWrapper(env)\n",
        "```\n",
        "\n",
        "- 2) Utiliser la politique d'exploration UCB (Upper-Confidence Bound) plutôt que epsilon-greedy. Cette politique est fondamentalement curieuse envers les couples (états, actions) qu'elle n'a jamais vu, et peut donc explorer la totalité de notre environnement beaucoup plus rapidement. Notez que la politique UCB est très efficace mais n'est pas applicable lorsque l'espace des états ou des actions est non discret. Pour rappel, la formule de la politique UCB est la suivante : $UCB(s, a) = Q(s, a) + c \\sqrt{\\frac{\\ln N(s)}{N(s, a)}}$ avec $N()$ dénotant le nombre de fois qu'un état ou qu'un couple (état, action) a été visité.\n",
        "\n",
        "- 3) Une méthode incitant l'exploration est l'Initialisation Optimiste. Cette méthode consiste à initialiser les valeurs de la fonction de valeur (ou Q-Values) à des valeurs optimistes. Les politiques qui se basent sur la Q-Value vont ainsi explorer davantage les actions qui n'ont pas encore été essayées, car elles commenceront avec une valeur Q élevée, puis celle-ci sera ajustée à la baisse au fur et à mesure de l'apprentissage, ce qui incite l'agent à passer à d'autres actions.\n",
        "\n",
        "Vous pouvez implémenter une ou plusieurs de ces solutions pour améliorer l'apprentissage de vos agents, puis run à nouveau la cellule précédente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A80Ac3AoR0vr"
      },
      "source": [
        "# Partie 4: Algorithme Deep Q-Network (DQN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9FoP3reMAXp"
      },
      "source": [
        "Il est temps de passer à un environnnement plus compliqué: Breakout (Casse-Briques).\n",
        "\n",
        "Cet environnement a un espace d'observation continu et de grande dimension (210x160x3 pixels). L'apprentissage de la Q-Value passera donc non plus par une table mais par un réseau de neurones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "2_LrPp4IA1T6",
        "outputId": "5a6f821e-df0a-4bf3-fee4-c505ef01886f"
      },
      "outputs": [],
      "source": [
        "env_breakout = env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
        "print(\"Observation space:\", env_breakout.observation_space)\n",
        "print(\"Action space:\", env_breakout.action_space)\n",
        "print(f\"Actions' meaning: {env_breakout.unwrapped.get_action_meanings()}\")\n",
        "obs,_ = env_breakout.reset()\n",
        "plt.imshow(env_breakout.render(),vmin=0,vmax=255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb07NvacMF4O"
      },
      "source": [
        "Nous allons utiliser des wrappers déjà fournis par Gymnasium pour:\n",
        "- processer l'image (réduire sa taille, tout mettre en gris...)\n",
        "- stacker les N dernières images (permettant de tracker le mouvement des objets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eEcTaEzDKF4"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK8p_dE9DLja"
      },
      "outputs": [],
      "source": [
        "env_breakout = env = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\", frameskip=1)\n",
        "env_breakout = AtariPreprocessing(env_breakout)\n",
        "env_breakout = FrameStackObservation(env_breakout, 4)\n",
        "env_breakout.reset()\n",
        "env_breakout.step(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "jvZDu9puDQNd",
        "outputId": "a12afbec-729c-4c98-9a08-8788fb72229a"
      },
      "outputs": [],
      "source": [
        "# Trying a random agent in Breakout\n",
        "agent = RandomAgent(env_breakout)\n",
        "\n",
        "# Evaluate the agent\n",
        "eval_results = eval_agent_n_episodes(env_breakout, agent, n=10)\n",
        "print(f\"Total reward: {eval_results['total_reward']}, mean = {np.mean(eval_results['total_reward'])}\")\n",
        "print(f\"Total steps: {eval_results['steps']}, mean = {np.mean(eval_results['steps'])}\")\n",
        "print(f\"Animation (last episode):\")\n",
        "eval_results['animation'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycHzHli5Mc4P"
      },
      "source": [
        "## B) Replay Buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ0xccD_Mfoq"
      },
      "source": [
        "Comme dit dans le cours, plutôt que d'apprendre totalement \"online\" (c'est-à-dire en utilisant la dernière transition collectée pour faire une mise à jour de la Q-Value), nous allons stocker les transitions dans un buffer, et faire des mises à jour en samplant aléatoirement des transitions dans ce buffer.\n",
        "\n",
        "Implémentons notre replay buffer permettant de récolter et de sampler aléatoirement des transitions collectées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZsEfOk4AODK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, device):\n",
        "        self.capacity = capacity # capacity of the buffer\n",
        "        self.data = []\n",
        "        self.index = 0 # index of the next cell to be filled\n",
        "        self.device = device\n",
        "    def append(self, obs, action, reward, next_obs, done):\n",
        "        if len(self.data) < self.capacity:\n",
        "            self.data.append(None)\n",
        "        self.data[self.index] = (obs, action, reward, next_obs, done)\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.data, batch_size)\n",
        "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n3x3PfgMpMG"
      },
      "source": [
        "## C) Réseau de neurones pour la Q-Value\n",
        "\n",
        "Nous allons utiliser un réseau de neurones convolutifs (car adapté aux images) à 3 couches pour approximer la Q-Value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMGBkPxMFwEF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AtariCNN(nn.Module):\n",
        "    def __init__(self, in_channels=4, n_actions=6):\n",
        "        super(AtariCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.head = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RLgzb7dM4Nh"
      },
      "source": [
        "A votre tour d'implémenter une fonction calculant les Q-values pour un state et jouant greedy à partir de ces Q-values (i.e. retournant l'index de la plus haute Q-value):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTxrUyqnEehb"
      },
      "outputs": [],
      "source": [
        "def greedy_action(network, obs):\n",
        "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
        "    with torch.no_grad():        \n",
        "        # ---- <your code here> ----\n",
        "        action = ...\n",
        "        # --------------------------\n",
        "    return action\n",
        "\n",
        "obs,_ = env_breakout.reset()\n",
        "network_q_value = AtariCNN()\n",
        "if torch.cuda.is_available():  # Set the DQN to cuda if possible\n",
        "    network_q_value = network_q_value.to(\"cuda\")\n",
        "    \n",
        "greedy_action(network_q_value, obs)   # test the function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVbF5-AxNh2Z"
      },
      "source": [
        "## D) Algorithme DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtwsDhtdNlvw"
      },
      "source": [
        "Il est temps d'implémenter l'entraînement de notre DQN.\n",
        "Nous allons utiliser:\n",
        "- une exploration epsilon-greedy avec un epsilon qui décroît linéairement\n",
        "- une phase de warm-up au début où l'agent joue aléatoirement pour remplir le replay buffer\n",
        "- un target Q-network pour stabiliser l'entrainement : il s'agit d'une copie du Q-network qui \"suit\" le Q-network principal plus lentement (par Polyak averaging). Le target Q-network est utilisé pour calculer la target dans la loss de DQN :\n",
        "$$\\text{target} = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')$$\n",
        "\n",
        "$$Q_{\\text{target}} \\leftarrow \\tau Q + (1-\\tau) Q_{\\text{target}}$$\n",
        "\n",
        "Avec $\\tau \\ll 1$.\n",
        "\n",
        "C'est à vous d'implémenter la méthode `act` et le calcul de la loss à partir des batch de transitions samplées dans le replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class DQNAgent(AgentRL):\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        env : gym.Env, \n",
        "        network_q_value : nn.Module,\n",
        "        replay_buffer : ReplayBuffer,\n",
        "        batch_size : int,\n",
        "        learning_rate : float,\n",
        "        gamma : float,\n",
        "        epsilon_start : float,\n",
        "        epsilon_end : float,\n",
        "        epsilon_decay_per_episode : int,\n",
        "        warmup_episodes : int = 20,\n",
        "        update_target_tau : float = 0.005,\n",
        "        device : str = \"cpu\",\n",
        "    ):\n",
        "        super().__init__(env)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.batch_size = batch_size   # batch size for sampling from the replay buffer and do a gradient step\n",
        "        self.gamma = gamma\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay_per_episode  # how much substract to epsilon per episode\n",
        "        self.gradient_step_freq = 4  # do a gradient step every gradient_step_freq steps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.warmup_episodes = warmup_episodes # number of episodes to fill the replay buffer before starting training\n",
        "        self.update_target_tau = update_target_tau  # target network update rate (Polyak averaging)\n",
        "        \n",
        "        # Internal variables\n",
        "        self.network_q_value = network_q_value.to(device)\n",
        "        self.network_q_value_target = deepcopy(self.network_q_value).to(device)\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.optimizer = torch.optim.Adam(self.network_q_value.parameters(), lr=self.learning_rate)\n",
        "        self.device = device\n",
        "        self.training_steps = 0  # number of training steps done so far\n",
        "        self.training_episodes = 0  # number of training episodes done so far\n",
        "        self.epsilon = epsilon_start\n",
        "        \n",
        "    def act(self, obs):\n",
        "        # ---- <your code here> ----\n",
        "        action = ...\n",
        "        # --------------------------\n",
        "    \n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        \n",
        "        # Store the transition in the replay buffer\n",
        "        self.replay_buffer.append(obs, action, reward, next_obs, done)\n",
        "        self.training_steps += 1\n",
        "        \n",
        "        # Linearly decay epsilon\n",
        "        if done:\n",
        "            self.training_episodes += 1\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "            self.epsilon = max(self.epsilon, self.epsilon_end)\n",
        "        \n",
        "        # Skip if not enough samples in the replay buffer or not time for a gradient step\n",
        "        if len(self.replay_buffer) < self.batch_size or self.training_steps % self.gradient_step_freq != 0:\n",
        "            return\n",
        "        \n",
        "        # Sample a batch of transitions from the replay buffer and do some conversions\n",
        "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = self.replay_buffer.sample(self.batch_size)\n",
        "        obs_batch = obs_batch / 255.0 # dim = (batch_size, 4, 84, 84)\n",
        "        next_obs_batch = next_obs_batch / 255.0 # dim = (batch_size, 4, 84, 84)\n",
        "        action_batch = action_batch.long().unsqueeze(1) # dim = (batch_size, 1)\n",
        "        reward_batch = reward_batch.unsqueeze(1)  # dim = (batch_size, 1)\n",
        "        done_batch = done_batch.unsqueeze(1)  # dim = (batch_size, 1)\n",
        "        \n",
        "        # Compute current Q values\n",
        "        # ---- <your code here> ----\n",
        "        current_q_values = ...\n",
        "        # --------------------------        \n",
        "        \n",
        "        # Compute target Q values\n",
        "        with torch.no_grad():\n",
        "            # ---- <your code here> ----\n",
        "            target_q_values = ...\n",
        "            # --------------------------\n",
        "            pass\n",
        "        \n",
        "        # Compute loss\n",
        "        # ---- <your code here> ----\n",
        "        loss = ... \n",
        "        # --------------------------\n",
        "        \n",
        "        \n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update the target network with Polyak averaging\n",
        "        with torch.no_grad():\n",
        "            for param, target_param in zip(self.network_q_value.parameters(), self.network_q_value_target.parameters()):\n",
        "                target_param.data.copy_(\n",
        "                    (1 - self.update_target_tau) * target_param.data + self.update_target_tau * param.data\n",
        "                )\n",
        "        \n",
        "            \n",
        "            \n",
        "\n",
        "# Train for 2 episodes here to debug\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "agent = DQNAgent(\n",
        "    env = env_breakout,\n",
        "    network_q_value = AtariCNN(\n",
        "        in_channels=env_breakout.observation_space.shape[0],\n",
        "        n_actions=env_breakout.action_space.n,\n",
        "    ),\n",
        "    replay_buffer = ReplayBuffer(capacity=100000, device=device),\n",
        "    batch_size = 64,\n",
        "    learning_rate = 0.001,\n",
        "    gamma = 0.95,\n",
        "    epsilon_start = 1.0,\n",
        "    epsilon_end = 0.1,\n",
        "    epsilon_decay_per_episode = 0.9 / 100,  # from 1.0 to 0.1 in 100 episodes\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "train_results = train_agent_n_episodes(\n",
        "    env = env_breakout,\n",
        "    agent = agent,\n",
        "    n = 2,\n",
        ")\n",
        "print(train_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4SLQftvdCi"
      },
      "source": [
        "C'est parti pour lancer l'entraînement de notre DQN !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOcRY7rsEoKz"
      },
      "outputs": [],
      "source": [
        "# Train the agent\n",
        "training_results = train_agent_n_episodes(env_breakout, agent, n=200, verbose_freq=10)\n",
        "\n",
        "# Plot training curves\n",
        "plot_results(training_results)\n",
        "\n",
        "# Evaluate the agent\n",
        "eval_results = eval_agent_n_episodes(env_breakout, agent, n=10)\n",
        "print(f\"Total reward: {eval_results['total_reward']}, mean = {np.mean(eval_results['total_reward'])}\")\n",
        "print(f\"Total steps: {eval_results['steps']}, mean = {np.mean(eval_results['steps'])}\")\n",
        "print(f\"Animation (last episode):\")\n",
        "eval_results['animation'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tips :\n",
        "- la loss `F.smooth_l1_loss` (aussi appelée Huber loss) est souvent plus stable que la MSE loss\n",
        "- une amélioration possible est la variante Double DQN, qui consiste à utiliser le Q-network principal pour choisir l'action maximisante dans la target, mais le target Q-network pour évaluer cette action. La target devient donc :\n",
        "$$\\text{target} = r + \\gamma Q_{\\text{target}}(s', \\arg\\max_{a'} Q(s', a'))$$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "A2YW1svvqjvG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
